{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMEANS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13gaT8XDJGVDb0aF2yMCVx81xVr6-OhSb",
      "authorship_tag": "ABX9TyMiepoIsvxWXp/paV00o8IN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mankicom/DEV_GDPS_TEMP_LSTM/blob/master/KMEANS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 지점개수 축약을 위한 군집분류"
      ],
      "metadata": {
        "id": "KjUJAUzhYW1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **모듈 로드**\n",
        "\n",
        "*   데이터 핸들 기본모듈: numpy, pandas, math, time, os, sys, copy\n",
        "*   데이터 정규화 모듈: MinMaxScaler\n",
        "*   K평균 분류 모듈: KMeans\n",
        "*   사용자 정의 모듈: tran_data_load_stnall_y(관측자료 로드 함수), find_stn_idx: 지점번호 idx 호출\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c0GESn4ujpwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Module load\n",
        "\n",
        "#.. module\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from time import time\n",
        "import os\n",
        "import sys\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import copy\n",
        "\n",
        "#.. local\n",
        "sys.path.insert(0, '/content/kmeans/inc')\n",
        "from tran_data_load_stnall_y import tran_data_load_stnall_y\n",
        "from test_find_stnidx import find_stn_idx\n",
        "#import read_namelistf90\n"
      ],
      "metadata": {
        "id": "F7Rr5d2QZnwX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **데이터 세트 설정**\n",
        "\n",
        "\n",
        "*   입력 데이터의 경로, 파일명, 차원 정보 설정\n",
        "*   kmeans 설정\n",
        "\n"
      ],
      "metadata": {
        "id": "OyUCurr1nfc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Data set\n",
        "\n",
        "data_dir = '/content/kmeans/DAIN/' # 입력자료 경로\n",
        "tran_data_per = \"2016050100-2021043000-24-1605-2104\" # 입력자료 파일의 기간명\n",
        "obs_stn_list = '/content/kmeans/DABA/new_dfs_merg_station_directory_2021050100.dat' # 지점정보 파일\n",
        "\n",
        "num_fct = 136     # 입력자료 예측시각 개수\n",
        "num_his = 1826    # 입력자료 표본개수\n",
        "num_ele = 2       # 입력자료 변수 개수 [u,v] 2개\n",
        "element = 'uvw'   # 입력자료 파일의 요소명\n",
        "\n",
        "# kmeans 분류개수 설정 및 결과 파일이름 설정\n",
        "n_clst = 4       \n",
        "out_file = '/content/kmeans/DAOU/kmean_k'+str(n_clst)+'out_nor_'+ element + '_' + tran_data_per + '_00utc'"
      ],
      "metadata": {
        "id": "VZcizEbpnvbu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **관측지점 정보 업로드**\n",
        "\n",
        "\n",
        "*   동네예보용 지점정보 형식 자료 로드 및 위경도, 지점번호, 지점개수 정보 획득\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLPEtIMXpHXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. OBS Station load\n",
        "\n",
        "print (\"2. Read OBS station list\")\n",
        "exists = os.path.isfile(obs_stn_list)\n",
        "if exists:\n",
        "   obs_stn_id  = pd.read_fwf(obs_stn_list, delimiter=' ', header=2, usecols=[0])\n",
        "   obs_stn_id  = np.array(obs_stn_id,  dtype=np.int)\n",
        "   obs_stn_lat  = pd.read_fwf(obs_stn_list, delimiter=' ', header=2, usecols=[3])\n",
        "   obs_stn_lat  = np.array(obs_stn_lat,  dtype=np.float)\n",
        "   obs_stn_lon  = pd.read_fwf(obs_stn_list, delimiter=' ', header=2, usecols=[4])\n",
        "   obs_stn_lon  = np.array(obs_stn_lon,  dtype=np.float)\n",
        "   num_stn = len(obs_stn_id)\n",
        "else:\n",
        "   sys.exit(\"STOP Error: Could not found : \"+ obs_stn_list)\n",
        "#print(obs_stn_id)\n",
        "\n",
        "# .. Find stn_id_idx\n",
        "#run_stn_idx = find_stn_idx(obs_stn_id, run_stn_id)"
      ],
      "metadata": {
        "id": "5nKZbC3VpTDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **입력자료 로드**\n",
        "\n",
        "\n",
        "*   관측 U,V 자료 로드\n",
        "*   -999. 값은 np.nan으로 대체\n",
        "*   자료가공 편의를 위해 업로드한 입력자료의 차원순서 변경 및 확인\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iV4_8SZLpQgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Fcst load :  data dim( input_size, num_stn, num_his, num_fct )\n",
        "#\n",
        "#    Trainining data used for cross-validation\n",
        "#    Test data used to evaluate best model out of randomized search\n",
        "#\n",
        "\n",
        "print (\"3. Training data load\")\n",
        "tran_y = tran_data_load_stnall_y( data_dir, tran_data_per, element,\n",
        "                                  num_ele, num_stn, num_his, num_fct )\n",
        "\n",
        "np.place(tran_y, tran_y==-999., np.nan)\n",
        "tran_y = np.swapaxes(tran_y, 0, 3)\n",
        "input_size = tran_y.shape[3]\n",
        "\n",
        "print(\"loaded data shape: \", tran_y.shape)\n",
        "\n",
        "input_size = tran_y.shape[3]\n",
        "print ('tran_y shape= ', tran_y.shape)    # sequence, stn, batch, feature"
      ],
      "metadata": {
        "id": "7l9rXaJHqSp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **데이터 정규화 및 평균**\n",
        "\n",
        "kmeans 입력자료 생성을 위한 데이터 전처리1: [지점별 기간평균, 변수] 차원으로 가공하기\n",
        "\n",
        "*   drop nan for nomalize for minmax scaler input\n",
        "> tran_y의 결측 제거 및 정규화를 위해 먼저 4차원[시간개수,지점개수,표본개수,변수개수] 변수를 2차원[*,변수개수]로 치환하여 새로운 변수 fit_tran_y로 복사.\n",
        "> numpy object인 fit_tran_y를 pandas object로 바꿔 pandas.DataFrame의 dropna(표본개수,특징)를 사용해 결측 행 제거. \n",
        "\n",
        "*   get restorator with obs range\n",
        "> 정규화 모듈 MinMaxScaler object 생성(obs_scaler) 및 fit_tran_y 피팅--> 피팅한 자료의 요소별 최대최소값 정보 산출\n",
        "\n",
        "*   feature nomalize\n",
        "> 지점을 분류하기 때문에 [각 지점의 평균값, 변수] 차원의 데이터를 만들어야 한다. 정규화 최종산출물 선언(avg_nor_tran_y[지점,변수]) 및 초기화 하고, for 문을 이용해 지점별로 결측을 제거하고, 정규화 한 뒤, [지점,변수] 차원을 제외한 나머지의 평균값을 구한다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "skuQU1FysUPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Normalize & mean\n",
        "\n",
        "# .. drop nan for normalize for minmax scaler input\n",
        "tr_q, tr_s, tr_b, tr_f = tran_y.shape[0], tran_y.shape[1], tran_y.shape[2], tran_y.shape[3]\n",
        "fit_tran_y = copy.deepcopy( tran_y.reshape(tr_q*tr_s*tr_b, tr_f) )\n",
        "print('before dropna, fit_tran_y shape: ', fit_tran_y.shape)\n",
        "fit_tran_y = pd.DataFrame(fit_tran_y)\n",
        "fit_tran_y = fit_tran_y.dropna()\n",
        "print('after dropna, fit_tran_y shape: ', fit_tran_y.shape)\n",
        "\n",
        "# .. restorator fitting\n",
        "obs_scaler = MinMaxScaler()   # copy default true\n",
        "obs_scaler.fit(fit_tran_y)\n",
        "\n",
        "\n",
        "# .. feature normalize   ( train seq, feature = test seq, feature )\n",
        "avg_nor_tran_y = np.ndarray( shape=(num_stn,tr_f), dtype=np.float32 )\n",
        "avg_nor_tran_y.fill(np.nan)\n",
        "print(\"-------Normalizing each stn\")\n",
        "for i in range(num_stn):\n",
        "      mis_idx = np.unique( np.argwhere(np.isnan(tran_y[:,i,:,:]))[:,1] )   # num_his-n\n",
        "      print(mis_idx.shape[0], tr_b)\n",
        "      if mis_idx.shape[0]==tr_b: continue\n",
        "      rem_tran_y = np.delete( tran_y[:,i,:,:], mis_idx, axis=1 )   # f, num_his-n, num_fct\n",
        "      rem_tr_q, rem_tr_b, rem_tr_f = rem_tran_y.shape[0], rem_tran_y.shape[1], rem_tran_y.shape[2]\n",
        "      nor_rem_tran_y = obs_scaler.transform(rem_tran_y.reshape(rem_tr_q*rem_tr_b, rem_tr_f))\n",
        "      print(\"nor_rem_tran_y shape: \", nor_rem_tran_y.shape)\n",
        "      avg_nor_tran_y[i,:] = np.nanmean( nor_rem_tran_y, axis=0 )"
      ],
      "metadata": {
        "id": "GCBVj-sAs4vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **지점 필터**\n",
        "\n",
        "kmeans 입력자료 생성을 위한 데이터 전처리2: 표본개수가 1년이 안되는 지점은 분석에서 제외\n",
        "\n",
        "*   분석지점 중 표본 개수가 365개 미만의 지점은 분석에서 제외\n",
        "> for 문을 통해 지점별로 표본의 개수가 365개 미만인 지점의 idx를 'missing_idx' 변수에 모아 위에서 구했던 avg_nor_tran_y 변수에서 그 지점들을 제거한다-->re_avg_nor_tran_y. 이와 동시에 지점번호, 위경도 정보를 똑같은 순서로 매칭을 해준다. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BC0TWODh5ioS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. drop nan\n",
        "\n",
        "print ('---------- Final training data shape before remove missing')\n",
        "print ('avg_nor tran obs : ', avg_nor_tran_y.shape)\n",
        "#print ('avg_tran obs : ', avg_tran_y)\n",
        "\n",
        "\n",
        "## .. filter: avg_nor = -999.\n",
        "#missing_idx = np.where( np.isnan(avg_nor_tran_y) )\n",
        "#print ( missing_idx )\n",
        "\n",
        "# .. filter: total - missing day < 365\n",
        "mis_obs_thr = []   # for station\n",
        "mis_tred = 365\n",
        "for i in range(num_stn):\n",
        "    mis_obs_idx = np.unique( np.argwhere( np.isnan(tran_y[:,i,:,0]) )[:,1] )\n",
        "\n",
        "    if num_his - len(mis_obs_idx) < mis_tred:\n",
        "       print ( i, obs_stn_id[i], \" obs: {}\".format(mis_obs_idx.shape), num_his-len(mis_obs_idx) )\n",
        "       mis_obs_thr.extend([i])\n",
        "missing_idx = sorted(set(mis_obs_thr))\n",
        "print(\"filtering station(total - missing day <365)\")\n",
        "print(missing_idx)\n",
        "print(\"missing len: \", len(missing_idx))\n",
        "print(\"total-missing: \", num_stn - len(missing_idx))\n",
        "\n",
        "\n",
        "re_avg_nor_tran_y = np.delete( avg_nor_tran_y, missing_idx, axis=0 )\n",
        "re_obs_stn_id = np.delete( obs_stn_id, missing_idx, axis=0 )\n",
        "re_obs_stn_lat = np.delete( obs_stn_lat, missing_idx, axis=0 )\n",
        "re_obs_stn_lon = np.delete( obs_stn_lon, missing_idx, axis=0 )\n",
        "\n",
        "print ('---------- Final training data shape after remove missing')\n",
        "print ('re_avg_tran obs : ', re_avg_nor_tran_y.shape)"
      ],
      "metadata": {
        "id": "cspdxqFH6IkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **k-means 분류 수행**\n",
        "\n",
        "\n",
        "*   KMeans 함수 설정 및 분류 수행: 분류개수(n_clusters)를 제외한 나머지 설정 디폴트 값 사용\n",
        "*   지점필터로 인한 분석 지점개수 재입력\n",
        "*   for 문을 통해 지점번호, 위도, 경도, 입력자료_u값, 입력자료_v값, 분류결과 파일로 출력\n",
        "*   출력값 동일하게 터미널로 확인\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r0vIbxa77ufI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Set Model\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clst, random_state=0).fit(re_avg_nor_tran_y)\n",
        "\n",
        "# .. print file\n",
        "num_stn = re_avg_nor_tran_y.shape[0]\n",
        "\n",
        "with open( out_file, 'w' ) as f:\n",
        "     for i in range(num_stn):\n",
        "          # .. for num_ele = 1\n",
        "          #print ( re_obs_stn_id[i][0], re_obs_stn_lat[i][0], re_obs_stn_lon[i][0], kmeans.labels_[i],\n",
        "          #        sep=',', file=f)\n",
        "          # .. for num_ele = 2\n",
        "          print ( re_obs_stn_id[i][0], re_obs_stn_lat[i][0], re_obs_stn_lon[i][0],\n",
        "                  re_avg_nor_tran_y[i][0], re_avg_nor_tran_y[i][1], kmeans.labels_[i], sep=',', file=f)\n",
        "\n",
        "for i in range(num_stn):\n",
        "    print ( re_obs_stn_id[i][0], re_obs_stn_lat[i][0], re_obs_stn_lon[i][0], kmeans.labels_[i],\n",
        "            sep=',')"
      ],
      "metadata": {
        "id": "on3e3AD_73NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **결과 예시**\n",
        "cvs 포맷으로 출력\n",
        "\n",
        "47090,38.25085,128.56473,0.48837212,0.7542534,3\n",
        "\n",
        "47093,37.947379999999995,127.75443,0.486916,0.75637555,3\n",
        "\n",
        "47095,38.147870000000005,127.3042,0.4876144,0.75541806,3\n"
      ],
      "metadata": {
        "id": "e198MCQV-95G"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian_search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7dbMTV9YJ+CnTmj/pmXTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mankicom/DEV_GDPS_TEMP_LSTM/blob/master/Bayesian_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlPCQwfvRr6K"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian optimization을 이용한 TCN 초모수 최적화**\n",
        "단기 풍속 편차보정모델 개발 예제\n",
        "\n"
      ],
      "metadata": {
        "id": "uL_Eoe-0Rs9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **모듈로드**"
      ],
      "metadata": {
        "id": "icJ2ynsfSEaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Module load\n",
        "\n",
        "#.. module\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from time import time\n",
        "\n",
        "from sklearn.metrics import make_scorer, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import joblib\n",
        "import argparse\n",
        "import f90nml\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.layers import Dense, TimeDistributed\n",
        "from tensorflow.keras import Input, Model, callbacks\n",
        "from tensorflow.keras.utils import plot_model as plm\n",
        "from tcn import TCN, tcn_full_summary\n",
        "from tensorflow.keras.activations import swish\n",
        "\n",
        "\n",
        "#.. local\n",
        "sys.path.insert(0, './inc')\n",
        "from tran_data_split import tran_data_split\n",
        "from kmk_make_scorer import r2_3dim, mse_3dim, mae_3dim"
      ],
      "metadata": {
        "id": "QWdun_yrR84U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "6HF7_f4lSH8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Device configuration\n",
        "\n",
        "# .. Set number of gpu\n",
        "#ap = argparse.ArgumentParser()\n",
        "#ap.add_argument(\"-g\", \"--gpus\", type=int, default=1,\n",
        "#                help=\"# of GPUS to use for training\")\n",
        "#args = vars(ap.parse_args())\n",
        "#G = args[\"gpus\"]\n",
        "#\n",
        "#print(\"[INFO] {} cores available\".format(G))\n",
        "\n",
        "\n",
        "\n",
        "# .. Memory set up\n",
        "#config = tf.ConfigProto()\n",
        "#config.gpu_options.allow_growth = True\n",
        "##config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
        "#K.set_session(tf.Session(config=config))\n",
        "\n",
        "#seed = 1\n",
        "#np.random.seed(seed)\n",
        "\n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "set_session(tf.compat.v1.Session(config=config))"
      ],
      "metadata": {
        "id": "nxypa7qgSIjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "2EP4_OrcSNWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Data set\n",
        "\n",
        "element = 'ALLV'\n",
        "name_list = \"./SHEL/namelist.input\"\n",
        "\n",
        "hp_lr = 0.009\n",
        "hp_pd = 'same'\n",
        "hp_ns = 1\n",
        "hp_dl = [1,2,4,8,17,34,68,136]\n",
        "\n",
        "n_iter_search = 20"
      ],
      "metadata": {
        "id": "kGsPP9V_SQR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "T0ZkenyKSRYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Read namelist\n",
        "\n",
        "print (\"1. Read namelist\")\n",
        "exists = os.path.isfile(name_list)\n",
        "if exists:\n",
        "    nml = f90nml.read(name_list)\n",
        "    tran_data_per = nml['data_set']['tran_data_per']\n",
        "    tran_num_his = nml['data_set']['tran_num_his']\n",
        "    test_data_per = nml['data_set']['test_data_per']\n",
        "    test_num_his = nml['data_set']['test_num_his']\n",
        "    num_fct = nml['data_set']['num_fct']\n",
        "    dev_stn_id = nml['data_set']['dev_stn_id']\n",
        "    exp_name = nml['data_set']['exp_name']\n",
        "    data_dir = nml['data_set']['data_dir']\n",
        "    input_size = nml['data_set']['input_size']\n",
        "    output_size = nml['data_set']['output_size']\n",
        "    num_epoch = nml['hyper_para']['num_epoch']\n",
        "    patience = nml['hyper_para']['patience']\n",
        "    hp_nf = nml['hyper_para']['n_filter']\n",
        "    hp_ks = nml['hyper_para']['s_kernel']\n",
        "    hp_dr = nml['hyper_para']['drp_rate']\n",
        "else:\n",
        "    sys.exit(\"STOP Error: Could not found : \"+ name_list)\n",
        "\n",
        "hp_ldl = hp_dl[-1] # last dilation factor to make name of save model\n",
        "hp_bn = True\n",
        "\n",
        "\n",
        "#data_dir = './DAIN_MIX/'\n",
        "#data_dir = './DAIN_HR136/'\n",
        "csv_outdir = './DAOU/LOSS/' + exp_name + '/'\n",
        "model_outdir = './DAOU/MODL/' + exp_name + '/'\n",
        "scalr_outdir = './DAOU/SCAL/' + exp_name + '/'\n",
        "gifd_outdir = './GIFD/' + exp_name + '/'"
      ],
      "metadata": {
        "id": "hdVvrp-dSSng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "Y2Av41tiSWm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Fcst load :  data dim( input_size, num_stn, num_his, num_fct )\n",
        "#\n",
        "#    Trainining data used for cross-validation\n",
        "#    Test data used to evaluate best model out of randomized search\n",
        "#\n",
        "\n",
        "print (\"3. Training/valid data load, combine 4stn(47169, 47133, 47102, 47090) train sample\")\n",
        "tran_rate = 0.8\n",
        "eval_rate = 0.2\n",
        "rd_seed_fix = False\n",
        "nbin = 10\n",
        "\n",
        "combine_stn = [47169, 47133, 47102, 47090]\n",
        "for i in range(len(combine_stn)):\n",
        "    tran_xx, tran_yy, test_xx, test_yy = tran_data_split(data_dir, tran_data_per,\n",
        "                                     element, input_size, output_size, tran_num_his,\n",
        "                                     num_fct, combine_stn[i], tran_rate, nbin, rd_seed_fix)\n",
        "\n",
        "    if i == 0:\n",
        "       tran_x, tran_y, test_x, test_y = tran_xx, tran_yy, test_xx, test_yy\n",
        "    else:\n",
        "       tran_x = np.concatenate((tran_x,tran_xx), axis=0)\n",
        "       tran_y = np.concatenate((tran_y,tran_yy), axis=0)\n",
        "       test_x = np.concatenate((test_x,test_xx), axis=0)\n",
        "       test_y = np.concatenate((test_y,test_yy), axis=0)"
      ],
      "metadata": {
        "id": "sinGzxOgSXia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "8S-4bL9GSaAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Select variable\n",
        "\n",
        "\n",
        "input_size = tran_x.shape[2]\n",
        "output_size = tran_y.shape[2]\n",
        "print (\"5. Select var\")\n",
        "print ('tran_x shape= ', tran_x.shape)    # batch, sequence, feature\n",
        "print ('tran_y shape= ', tran_y.shape)\n",
        "print ('test_x shape= ', test_x.shape)    # batch, sequence, feature\n",
        "print ('test_y shape= ', test_y.shape)\n"
      ],
      "metadata": {
        "id": "WpeR_DTcSa6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "Usidzi02Sc_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Normalize\n",
        "\n",
        "# .. initialaize\n",
        "tr_b, tr_s, tr_f = tran_x.shape[0], tran_x.shape[1], tran_x.shape[2]\n",
        "ts_b, ts_s, ts_f = test_x.shape[0], test_x.shape[1], test_x.shape[2]\n",
        "\n",
        "# .. get restorator with obs range\n",
        "nwp_scaler = MinMaxScaler()   # copy default true\n",
        "obs_scaler = MinMaxScaler()\n",
        "nwp_scaler.fit(tran_x.view().reshape(tr_b*tr_s, tr_f))\n",
        "obs_scaler.fit(tran_y.view().reshape(tr_b*tr_s, output_size))\n",
        "\n",
        "# .. feature normalize   ( train seq, feature = test seq, feature )\n",
        "nor_tran_x = nwp_scaler.transform(tran_x.reshape(tr_b*tr_s, tr_f))\n",
        "nor_tran_x = nor_tran_x.reshape(tr_b,tr_s,tr_f)\n",
        "nor_tran_y = obs_scaler.transform(tran_y.reshape(tr_b*tr_s, output_size))\n",
        "nor_tran_y = nor_tran_y.reshape(tr_b,tr_s, output_size)\n",
        "\n",
        "nor_test_x = nwp_scaler.transform(test_x.reshape(ts_b*ts_s, ts_f))\n",
        "nor_test_x = nor_test_x.reshape(ts_b,ts_s,ts_f)\n",
        "nor_test_y = obs_scaler.transform(test_y.reshape(ts_b*ts_s, output_size))\n",
        "nor_test_y = nor_test_y.reshape(ts_b,ts_s, output_size)\n",
        "\n",
        "\n",
        "print ('---------- Final training data shape')\n",
        "print(type(nor_tran_x))\n",
        "print ('tran nwp : ', nor_tran_x.shape)\n",
        "print ('tran obs : ', nor_tran_y.shape)\n",
        "print ('test nwp : ', nor_test_x.shape)\n",
        "print ('test obs : ', nor_test_y.shape)"
      ],
      "metadata": {
        "id": "tvoEuyKISf4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "ofabSskzShCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#=========================================================================\n",
        "# .. Model configuration\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "# .. Set batch for cross-validation\n",
        "\n",
        "if eval_rate == 0.2: num_cv=5\n",
        "#if eval_rate == 0.2: num_cv=2\n",
        "real_train_size = nor_tran_x.shape[0]\n",
        "batch_size = int( real_train_size*(1-eval_rate)*0.1 )\n",
        "#batch_size = real_train_size\n",
        "#batch_size = 572\n",
        "\n",
        "print ('input_size: ', input_size)\n",
        "print ('batch_size: ', batch_size)\n",
        "print ('time_lenght: ', num_fct)"
      ],
      "metadata": {
        "id": "qFV222rASjiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "v-LX9A7rSklj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Set Model\n",
        "\n",
        "\n",
        "# .. Define model\n",
        "def create_model(dropout_rate=0.15, nb_filters=7, kernel_size=3):\n",
        "\n",
        "          print ('================== Model called ========================')\n",
        "          print ('input_size: ', input_size)\n",
        "          print ('batch_size: ', batch_size)\n",
        "          print ('time_lenght: ', num_fct)\n",
        "          print ('nb_filters: ', nb_filters)\n",
        "          print ('kernel_size: ', kernel_size)\n",
        "          print ('dropout_rate: ', dropout_rate)\n",
        "          print ('dilations: ', hp_dl)\n",
        "          dropout_rate = np.round(dropout_rate,2)\n",
        "          print ('dropout_rate: ', dropout_rate)\n",
        "\n",
        "          ## .. clear keras model\n",
        "          K.clear_session()\n",
        "\n",
        "          # .. create model\n",
        "          #i = Input( batch_shape=(batch_size, num_fct, input_size) )\n",
        "          i = Input( batch_shape=(None, num_fct, input_size) )\n",
        "          o = TCN(return_sequences=True,\n",
        "                  activation=swish,\n",
        "                  nb_filters=nb_filters,\n",
        "                  padding=hp_pd,\n",
        "                  use_batch_norm = hp_bn,\n",
        "                  nb_stacks=hp_ns,\n",
        "                  dropout_rate=dropout_rate,\n",
        "                  kernel_size=kernel_size,\n",
        "                  use_skip_connections=True,\n",
        "                  dilations=hp_dl\n",
        "                  )(i)\n",
        "          o = TimeDistributed(Dense(output_size, activation='linear'))(o)\n",
        "\n",
        "          # .. compile\n",
        "          adam = optimizers.Adam(lr=hp_lr)\n",
        "\n",
        "          return m\n",
        "\n",
        "\n",
        "# .. Wrapping create_model for searching hyper-parameter\n",
        "model = KerasRegressor(build_fn=create_model,\n",
        "                       verbose=1,\n",
        "                       epochs=num_epoch,\n",
        "                       batch_size=batch_size,\n",
        "                       shuffle=True)"
      ],
      "metadata": {
        "id": "wGsiZR-iSokh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "4IfZLLB2SsV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------\n",
        "# .. Use bayes_opt\n",
        "\n",
        "# .. Exp para set\n",
        "#param_dist = { 'padding': Categorical(['causal','same']),\n",
        "#               'nb_stacks': Integer(1,5),\n",
        "#               'nb_filters': Categorical([7,20,30]),\n",
        "#               'kernel_size': Integer(2,24) }\n",
        "#param_dist = { 'dropout_rate': Real(0.01, 0.2),\n",
        "#               'nb_filters': Integer(50,100),\n",
        "#               'kernel_size': Integer(3,12) }\n",
        "##param_dist = { 'dropout_rate': Categorical([0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.15]),\n",
        "#               'nb_filters': Integer(50,100),\n",
        "#               'kernel_size': Integer(3,12) }\n",
        "param_dist = { 'nb_filters': Integer(50,100),\n",
        "               'kernel_size': Integer(3,12) }\n",
        "\n",
        "set_eval_score = { 'MAE': make_scorer(mae_3dim),\n",
        "                   'MSE': make_scorer(mse_3dim),\n",
        "                   'R2': make_scorer(r2_3dim) }\n",
        "\n",
        "                   print ( param_dist )\n",
        "\n",
        "optimizer =  BayesSearchCV( estimator=model,\n",
        "                            search_spaces=param_dist,\n",
        "                            scoring=make_scorer(r2_3dim),\n",
        "                            refit=False,\n",
        "                            cv=num_cv,\n",
        "                            n_iter=n_iter_search,\n",
        "                            return_train_score=True,\n",
        "                            verbose=1,\n",
        "                            n_jobs=1,\n",
        "                            random_state=1 )\n",
        "\n",
        "start = time()\n",
        "print(nor_tran_x.shape, nor_tran_y.shape)\n",
        "optimizer.fit(nor_tran_x, nor_tran_y)\n",
        "\n",
        "print(type(optimizer.cv_results_))\n",
        "print(optimizer.cv_results_)\n",
        "\n",
        "# .. Report\n",
        "def report(result, n_top=n_iter_search):\n",
        "    for i in range(n_top):\n",
        "        candidates = [ result['rank_test_score'][i] ]\n",
        "        for candidate in candidates:\n",
        "            print(\"Rank: %0d, R2: %.3f with %r\" %\n",
        "                  ( i, result['mean_test_score'][candidate-1],\n",
        "                       result['params'][candidate-1] ) )\n",
        "\n",
        "\n",
        "print(\"BayesSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings. \" % ((time() - start), n_iter_search))\n",
        "\n",
        "print( \"Best: %f using %s\" % ( optimizer.best_score_,\n",
        "                               optimizer.best_params_ ) )\n",
        "\n",
        "report(optimizer.cv_results_)"
      ],
      "metadata": {
        "id": "yxjPENFBStS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ㅁㄴㅇㄹ**"
      ],
      "metadata": {
        "id": "anUcUN8sS3Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#=========================================================================\n",
        "# .. Second refit to evaluate best model use test period\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "# .. Set model label\n",
        "\n",
        "# .. best model configuration for whole train set\n",
        "params = optimizer.best_params_\n",
        "print(type(params))\n",
        "print(params)"
      ],
      "metadata": {
        "id": "I2LjzM9ES4L3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}